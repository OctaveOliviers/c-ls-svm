\setcounter{page}{1}
\newpage
\section{SIF: Smooth Inverse Frequency for sentence embedding}

%\begin{mdframed}

\textbf{Short description}

We compute the embedding $c_s$ of sentence $s$ in two steps:
\begin{enumerate}[topsep=0pt, itemsep=0pt]
    \item \textbf{Compute the MLE $\Tilde{c}_s$} as a weigthed average of the word embeddings $v_w$.
    \begin{equation*}
        \Tilde{c}_s \propto \sum_{w \in s} \dfrac{a}{a + p(w)} v_w
    \end{equation*}
    with $a$ a parameter and $p(w)$ the word frequency.
        
    \item \textbf{Substract the first principal component $c_0$}, computed from a set of $\Tilde{c}_s$'s.
    \begin{equation*}
        c_s = \Tilde{c}_s - c_0
    \end{equation*}
        
\end{enumerate}

This way, the embedding represents a random walk through the embedding space wherein nearby words are generated in related sentences.
Besides, $c_0$ embodies common words with a small discriminating power such as \textit{just} \textit{when} or \textit{there}.


    

\textbf{What we should retain}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] Filter out irrelevant words ($\sim$ noise) with a mathematically underpinned algorithm such as PCA.
    
    \item[-] 
    
    \item[-]
    
\end{enumerate}


\textbf{What we could improve}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] Combine the advantages of this unsupervised, computationally-light approach with models that take into account word order.
    
    \item[-] Extend the linear PCA to kernel PCA in order to take into account non-linearities from the dataset.
    
    \item[-]
    
\end{enumerate}


\textbf{Citation}

    Sanjeev Arora, Yingyu Liang and Tengyu Ma.
    A simple but tough-to-beat baseline for sentence embeddings.
    \textit{International Conference on Learning Representations}, 2017.

\url{https://openreview.net/pdf?id=SyK00v5xx}

%\end{mdframed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section{TF-KLD}

%\begin{mdframed}

\textbf{Short description}

We predict whether two sentences are related in three steps:
\begin{enumerate}[topsep=0pt, itemsep=0pt]
    \item \textbf{Compute the term-frequency matrix} $W \in \mathbb{R}^{M \times N}$ with $M$ the number of questions and $N$ the number of features.
    
    \item \textbf{Re-weight the counts with the Kullback-Leibler difergence} that indicates the discriminating power of each feature.
    \begin{equation*}
        \begin{aligned}
        KL(p_k || q_k) &= \sum_{i=0}^M p_k(i) log\dfrac{p_k(i)}{q_k(i)}\\
        p_k(i) &= P(w_{ik}^{(1)}=1 \; | \; w_{ik}^{(2)} = 1, r_i=1)\\
        q_k(i) &= P(w_{ik}^{(1)}=1 \; | \; w_{ik}^{(2)} = 1, r_i=0)
        \end{aligned}
    \end{equation*}
    where $w_{ik}^{(1)}$ is feature $k$ of the first sentence in the $i^{th}$ pair and $r_i$ indicates whether both sentences are related or not.
    
    \item \textbf{Factorize the re-weighted matrix $W$ with SVD} to obtain a latent representation in which related sentences are similar.
    
\end{enumerate}
This supervised method trains a linear SVM to predict the relatedness of test questions based on their latent representations.

\textbf{What we should retain}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] This simple and low-cost approach offers a probabilistic framework that suggests the importance of each word in the sentence. Could we link this with attention ?
    
    \item[-] The kernel matrix and its factorization contains plenty of information on the structure in the data set (principal components, number of clusters). 
    
    \item[-]
    
\end{enumerate}


\textbf{What we could improve}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] Extend this approach with a model that can generalize well in high-dimensional spaces based on few examples (SVD).
    
    \item[-] We could use the variance along the principal components of the data-cloud as attention weights. The importance of a word or group of words is given by the inner product of each word with the principal components.
    Could we combine the meaning of multiple words by projecting this group on some principal components ?
    
    \item[-]
    
\end{enumerate}


\textbf{Citation}

    Yangfeng Ji and Jacob Eisenstein.
    Discriminative improvements to distributional sentence similarity.
    \textit{Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing}, 891-896, 2013.

\url{https://pdfs.semanticscholar.org/c31b/d4b7dbdb0726fb78317f0f9f41e9eac7f78e.pdf}

%\end{mdframed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{Deep sentence embeddings with LSTM's}

%\begin{mdframed}

\textbf{Short description}

This and that and this

    

\textbf{What we should retain}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] 
    
    \item[-] 
    
    \item[-]
    
\end{enumerate}


\textbf{What we could improve}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] We could use the difference between two subsequent hidden states instead of the hidden states themselves.
    
    \item[-] 
    
    \item[-]
    
\end{enumerate}


\textbf{Citation}

    Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song and Rabab Ward.
    Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval.
    \textit{Arxiv:1502.06922}, 2016

\url{https://arxiv.org/pdf/1502.06922.pdf}

%\end{mdframed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{Siamese architecture}

%\begin{mdframed}

\textbf{Short description}

We process the two sentences with the same LSTM. The classification follows form a simple Manhattan metric on the hidden states.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.4]{pictures/Siamese_arch.png}
\end{figure}


\textbf{What we should retain}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] This simple architecture reaches a high performance with few parameters to train.
    
    \item[-] 
    
    \item[-]
    
\end{enumerate}


\textbf{What we could improve}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] We could compare the performance of the model that classifies based on hidden states (short-term information) and cell state (long-term information).
    
    \item[-] We could stack multiple LSTM's to obtain high-level representations of the sentence.
    
    \item[-]
    
\end{enumerate}


\textbf{Citation}

    Jonas Mueller and Aditya Thyagarajan.
    Siamese Recurrent Architectures for Learning Sentence Similarity.
    \textit{Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence}, 2786-2792, 2016.


\url{https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12195/12023}

%\end{mdframed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section{ELMo: contextualized word embeddings}

%\begin{mdframed}

\textbf{Short description}

This and that and this


\textbf{What we should retain}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] 
    
    \item[-] 
    
    \item[-]
    
\end{enumerate}


\textbf{What we could improve}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] 
    
    \item[-] 
    
    \item[-]
    
\end{enumerate}


\textbf{Citation}

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee and Luke Zettlemoyer.
Deep contextualized word representations.
\textit{Arxiv: 1802.05365}, 2018.


\url{https://arxiv.org/pdf/1802.05365.pdf}

%\end{mdframed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{CAS: Cell-Aware Stacked LSTM's}

%\begin{mdframed}

\textbf{Short description}

This and that and this


\textbf{What we should retain}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] 
    
    \item[-] 
    
    \item[-]
    
\end{enumerate}


\textbf{What we could improve}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] 
    
    \item[-] 
    
    \item[-]
    
\end{enumerate}


\textbf{Citation}

Jihun Choi, Taeuk Kim and Sang-goo Lee.
Cell-aware Stacked LSTMs for Modeling Sentences.
\textit{Arxiv: 1809.02279}, 2018.


\url{https://arxiv.org/pdf/1809.02279.pdf}

%\end{mdframed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{Transformer: replacing recurrence with attention}

%\begin{mdframed}

\textbf{Short description}

This and that and this


\textbf{What we should retain}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] 
    
    \item[-] 
    
    \item[-]
    
\end{enumerate}


\textbf{What we could improve}

\begin{enumerate}[topsep = 0pt, itemsep = 0pt]
    \item[-] 
    
    \item[-] 
    
    \item[-]
    
\end{enumerate}


\textbf{Citation}

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.
Attention is all you need.
\textit{Arxiv: 1706.03762}, 2017.


\url{https://arxiv.org/pdf/1706.03762.pdf}

%\end{mdframed}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%